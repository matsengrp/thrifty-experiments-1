{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from shmex.shm_data import pcp_df_of_non_shmoof_nickname, dataset_dict\n",
    "from netam.multihit import (\n",
    "    MultihitBurrito,\n",
    "    train_test_datasets_of_pcp_df,\n",
    "    HitClassModel,\n",
    "    prepare_pcp_df,\n",
    ")\n",
    "from netam.molevol import reshape_for_codons\n",
    "from epam import evaluation\n",
    "import netam.framework as framework\n",
    "import torch\n",
    "import pandas as pd\n",
    "from netam.common import BASES_AND_N_TO_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use a hit class crepe:\n",
    "Load some data and an existing crepe whose predictions should be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/wdumm/data/v1/tang-deepshm-oof_pcp_2024-04-09_MASKED_NI.csv.gz\n"
     ]
    }
   ],
   "source": [
    "burrito_params = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"min_learning_rate\": 1e-4,\n",
    "    # \"l2_regularization_coeff\": 1e-6\n",
    "}\n",
    "epochs = 200\n",
    "site_count = 500\n",
    "\n",
    "# This is the model whose predictions will be adjusted\n",
    "# nt_crepe_path = \"../train/trained_models/cnn_joi_lrg-shmoof_small-fixed-0\"\n",
    "nt_crepe_path = '../train/fixed_models/cnn_ind_med-shmoof_small-full-0'\n",
    "nt_crepe = framework.load_crepe(nt_crepe_path)\n",
    "\n",
    "# This is the hit class model\n",
    "hc_crepe_path = \"shmoof_small-hc\"\n",
    "hc_crepe = framework.load_crepe(hc_crepe_path)\n",
    "\n",
    "tang_df = pcp_df_of_non_shmoof_nickname(\"tangshm\")\n",
    "subsampled_tang_df = tang_df.copy().reset_index(drop=True)\n",
    "pcp_df = prepare_pcp_df(subsampled_tang_df, nt_crepe, site_count)\n",
    "\n",
    "train_data, val_data = train_test_datasets_of_pcp_df(pcp_df)\n",
    "starting_branch_lengths_estimates = train_data.branch_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model adjustments are produced with joint_train (they're not just based on the branch lengths for the uncorrected model),\n",
    "so you may want to re-fit branch lengths on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8997, 1.4292, 3.2327], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing branch lengths: 100%|██████████| 1997/1997 [19:49<00:00,  1.68it/s]    \n"
     ]
    }
   ],
   "source": [
    "burrito = MultihitBurrito(train_data, val_data, hc_crepe.model, **burrito_params)\n",
    "print(burrito.model.values.exp())\n",
    "new_branch_lengths = burrito.find_optimal_branch_lengths(train_data)\n",
    "train_data.branch_lengths = new_branch_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But if you just want to use the model to adjust your neutral codon prob predictions, you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce some random codon probs\n",
    "sample_codon_probs = torch.rand((100, 4, 4, 4))\n",
    "sample_codon_probs /= sample_codon_probs.sum(dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "# Make some random parent codon nt indices between 0 and 3:\n",
    "parent_codon_nt_indices = torch.randint(0, 4, (100, 3))\n",
    "\n",
    "# Here are your adjusted codon probs!\n",
    "adjusted_codon_probs = hc_crepe.model(parent_codon_nt_indices, sample_codon_probs.log()).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netam.hit_class as hit_class\n",
    "adjusted_aggregated = hit_class.hit_class_probs_tensor(parent_codon_nt_indices, adjusted_codon_probs)\n",
    "original_aggregated = hit_class.hit_class_probs_tensor(parent_codon_nt_indices, sample_codon_probs)\n",
    "\n",
    "original_log_probs = original_aggregated.log()\n",
    "corrections = torch.cat([torch.tensor([0.0]), hc_crepe.model.values])\n",
    "# we'll use the corrections to adjust the uncorrected hit class probs\n",
    "corrections = corrections[\n",
    "    torch.arange(4).unsqueeze(0).tile((original_log_probs.shape[0], 1))\n",
    "]\n",
    "original_log_probs += corrections\n",
    "aggregate_first = torch.softmax(original_log_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted tensor([1.4226e-04, 7.2570e-02, 2.8584e-01, 6.4145e-01],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "original tensor([2.9686e-04, 1.6832e-01, 4.1734e-01, 4.1404e-01])\n",
      "aggregate_first tensor([1.4226e-04, 7.2570e-02, 2.8584e-01, 6.4145e-01],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"adjusted\", adjusted_aggregated[0])\n",
    "print(\"original\", original_aggregated[0])\n",
    "print(\"aggregate_first\", aggregate_first[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
