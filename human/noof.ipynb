{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noof\n",
    "\n",
    "The `NoofModel` is a model that uses a transformer on the embeddings. \n",
    "It's pretty terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "from netam import framework, models\n",
    "from netam.common import pick_device, PositionalEncoding\n",
    "\n",
    "from shmex.shm_data import train_val_dfs_of_nickname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting shmoof_small as a shmoof dataset\n",
      "Using Metal Performance Shaders\n",
      "we have 46391 training examples and 2625 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 3\n",
    "site_count = 410\n",
    "\n",
    "train_df, val_df = train_val_dfs_of_nickname('shmoof_small')\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, site_count=site_count)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, site_count=site_count)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoofModel(models.KmerModel):\n",
    "    def __init__(\n",
    "        self, kmer_length, embedding_dim, nhead, dim_feedforward, layer_count, dropout_prob=0.3\n",
    "    ):\n",
    "        super().__init__(kmer_length)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.kmer_embedding = nn.Embedding(self.kmer_count, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=dropout_prob)\n",
    "\n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=self.embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = TransformerEncoder(self.encoder_layer, layer_count)\n",
    "        self.linear = nn.Linear(self.embedding_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, encoded_parents, masks, wt_base_modifier):\n",
    "        \"\"\"\n",
    "        The forward method.\n",
    "\n",
    "        encoded_parents is expected to be an integer tensor of [batch_size, sequence_length].\n",
    "        \"\"\"\n",
    "        kmer_embeddings = self.kmer_embedding(encoded_parents)\n",
    "        kmer_embeddings = self.pos_encoder(kmer_embeddings)\n",
    "\n",
    "        # Pass through the transformer encoder\n",
    "        transformer_output = self.encoder(kmer_embeddings)\n",
    "\n",
    "        # Apply the linear layer and squeeze out the last dimension.\n",
    "        # After the linear layer, the dimensions will be [batch_size, sequence_length, 1].\n",
    "        # We squeeze out the last dimension to make it [batch_size, sequence_length].\n",
    "        log_rates = self.linear(transformer_output).squeeze(-1)\n",
    "        rates = torch.exp(log_rates)\n",
    "        return rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 57/100 [1:01:51<46:39, 65.11s/it, loss_diff=2.161e-07, lr=6.25e-5, val_loss=0.05661]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.056811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.064110</td>\n",
       "      <td>0.056720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.063948</td>\n",
       "      <td>0.056638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063859</td>\n",
       "      <td>0.056569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063806</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.063779</td>\n",
       "      <td>0.056607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.063753</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063719</td>\n",
       "      <td>0.056609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063701</td>\n",
       "      <td>0.056592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.063670</td>\n",
       "      <td>0.056665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.063650</td>\n",
       "      <td>0.056583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.063640</td>\n",
       "      <td>0.056544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.063635</td>\n",
       "      <td>0.056633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.063626</td>\n",
       "      <td>0.056679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.063613</td>\n",
       "      <td>0.056625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.056629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.063604</td>\n",
       "      <td>0.056594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.063596</td>\n",
       "      <td>0.056575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.063588</td>\n",
       "      <td>0.056572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.063590</td>\n",
       "      <td>0.056556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.056639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.056571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.063575</td>\n",
       "      <td>0.056588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.063567</td>\n",
       "      <td>0.056639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.063568</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.063570</td>\n",
       "      <td>0.056619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.063569</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.063564</td>\n",
       "      <td>0.056642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.063568</td>\n",
       "      <td>0.056570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.056662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.063557</td>\n",
       "      <td>0.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.063557</td>\n",
       "      <td>0.056589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.056597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.056611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.056564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.063553</td>\n",
       "      <td>0.056599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.063547</td>\n",
       "      <td>0.056598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.056604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.063551</td>\n",
       "      <td>0.056602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.063557</td>\n",
       "      <td>0.056586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.063547</td>\n",
       "      <td>0.056591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.063546</td>\n",
       "      <td>0.056611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.063543</td>\n",
       "      <td>0.056619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.063548</td>\n",
       "      <td>0.056592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.063554</td>\n",
       "      <td>0.056597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.063548</td>\n",
       "      <td>0.056587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.063543</td>\n",
       "      <td>0.056603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.063540</td>\n",
       "      <td>0.056593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.063546</td>\n",
       "      <td>0.056610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.063547</td>\n",
       "      <td>0.056596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.063552</td>\n",
       "      <td>0.056607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.063549</td>\n",
       "      <td>0.056589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.056606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.063535</td>\n",
       "      <td>0.056607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.063542</td>\n",
       "      <td>0.056608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss\n",
       "0     0.064748  0.056811\n",
       "1     0.064110  0.056720\n",
       "2     0.063948  0.056638\n",
       "3     0.063859  0.056569\n",
       "4     0.063806  0.056598\n",
       "5     0.063779  0.056607\n",
       "6     0.063753  0.056598\n",
       "7     0.063719  0.056609\n",
       "8     0.063701  0.056592\n",
       "9     0.063694  0.056598\n",
       "10    0.063670  0.056665\n",
       "11    0.063650  0.056583\n",
       "12    0.063640  0.056544\n",
       "13    0.063635  0.056633\n",
       "14    0.063626  0.056679\n",
       "15    0.063613  0.056625\n",
       "16    0.063620  0.056600\n",
       "17    0.063616  0.056629\n",
       "18    0.063604  0.056594\n",
       "19    0.063596  0.056575\n",
       "20    0.063588  0.056572\n",
       "21    0.063590  0.056556\n",
       "22    0.063581  0.056639\n",
       "23    0.063581  0.056571\n",
       "24    0.063575  0.056588\n",
       "25    0.063567  0.056639\n",
       "26    0.063568  0.056598\n",
       "27    0.063570  0.056619\n",
       "28    0.063569  0.056598\n",
       "29    0.063564  0.056642\n",
       "30    0.063568  0.056570\n",
       "31    0.063559  0.056662\n",
       "32    0.063557  0.056601\n",
       "33    0.063557  0.056589\n",
       "34    0.063555  0.056597\n",
       "35    0.063559  0.056611\n",
       "36    0.063555  0.056564\n",
       "37    0.063553  0.056599\n",
       "38    0.063547  0.056598\n",
       "39    0.063559  0.056604\n",
       "40    0.063551  0.056602\n",
       "41    0.063557  0.056586\n",
       "42    0.063547  0.056591\n",
       "43    0.063546  0.056611\n",
       "44    0.063543  0.056619\n",
       "45    0.063548  0.056592\n",
       "46    0.063554  0.056597\n",
       "47    0.063548  0.056587\n",
       "48    0.063543  0.056603\n",
       "49    0.063540  0.056593\n",
       "50    0.063546  0.056610\n",
       "51    0.063547  0.056596\n",
       "52    0.063552  0.056607\n",
       "53    0.063549  0.056589\n",
       "54    0.063538  0.056606\n",
       "55    0.063535  0.056607\n",
       "56    0.063542  0.056608"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NoofModel(kmer_length=kmer_length, embedding_dim=10, nhead=2, dim_feedforward=256, layer_count=2, dropout_prob=0.1)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "model.to(device)\n",
    "\n",
    "burrito = framework.SHMBurrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.001, weight_decay=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
