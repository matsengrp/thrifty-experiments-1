{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from netam import framework, models\n",
    "from netam.common import pick_device, print_parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shmoof_data_path = \"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\"\n",
    "shmoof_data_path = \"/Users/matsen/data/shmoof_pcp_2023-11-30_MASKED.csv\"\n",
    "train_df, val_df = framework.load_shmoof_dataframes(shmoof_data_path, val_nickname=\"51\") # , sample_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "we have 26592 training examples and 22424 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 3\n",
    "site_count = 500\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, site_count=site_count)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, site_count=site_count)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Best Hyperparameters: {'kernel_size': 11, 'dropout': 0.3, 'embedding_dim': 7, 'num_filters': 19}\n",
    "model = models.CNNModel(train_dataset, embedding_dim=7, num_filters=19, kernel_size=11, dropout_prob=0.3)\n",
    "print_parameter_count(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "burrito = framework.SHMBurrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-5, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)\n",
    "torch.save(model, \"_ignore/cnn_lrg.pt\")\n",
    "losses.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 11:26:16,972] A new study created in memory with name: no-name-76a13ef0-a0eb-4935-9cd2-79d445b58da5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [02:45<00:00,  5.53s/it, loss_diff=-6.115e-05, lr=0.1, val_loss=0.06358]\n",
      "[I 2023-12-08 11:29:08,105] Trial 0 finished with value: 0.0635777899608032 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 0 with value: 0.0635777899608032.\n",
      "Epoch: 100%|██████████| 30/30 [02:52<00:00,  5.76s/it, loss_diff=-3.052e-06, lr=0.1, val_loss=0.0657] \n",
      "[I 2023-12-08 11:32:06,382] Trial 1 finished with value: 0.0656961277525977 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 0 with value: 0.0635777899608032.\n",
      "Epoch: 100%|██████████| 30/30 [03:52<00:00,  7.73s/it, loss_diff=-2.809e-06, lr=0.004, val_loss=0.06046]\n",
      "[I 2023-12-08 11:36:04,774] Trial 2 finished with value: 0.0604385759999103 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:29<00:00,  6.99s/it, loss_diff=-1.051e-05, lr=0.1, val_loss=0.06558]\n",
      "[I 2023-12-08 11:39:40,589] Trial 3 finished with value: 0.06558446325640827 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:06<00:00,  6.23s/it, loss_diff=-5.373e-05, lr=0.1, val_loss=0.06689]\n",
      "[I 2023-12-08 11:42:53,454] Trial 4 finished with value: 0.06587978616978839 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:51<00:00,  7.73s/it, loss_diff=-1.407e-06, lr=0.02, val_loss=0.06047]\n",
      "[I 2023-12-08 11:46:51,613] Trial 5 finished with value: 0.060467055409646996 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:57<00:00,  7.90s/it, loss_diff=-2.567e-05, lr=0.1, val_loss=0.06621]\n",
      "[I 2023-12-08 11:50:55,289] Trial 6 finished with value: 0.06620903523147766 and parameters: {'l2_regularization_coeff': 1e-06}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:08<00:00,  6.28s/it, loss_diff=-2.295e-05, lr=0.1, val_loss=0.06762]\n",
      "[I 2023-12-08 11:54:10,390] Trial 7 finished with value: 0.06607028753857208 and parameters: {'l2_regularization_coeff': 1e-06}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:31<00:00,  7.04s/it, loss_diff=-9.032e-05, lr=0.1, val_loss=0.06873]\n",
      "[I 2023-12-08 11:57:47,177] Trial 8 finished with value: 0.06657708030495735 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:47<00:00,  7.57s/it, loss_diff=-9.816e-05, lr=0.1, val_loss=0.06215]\n",
      "[I 2023-12-08 12:01:40,627] Trial 9 finished with value: 0.06214627420614117 and parameters: {'l2_regularization_coeff': 1e-05}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:31<00:00,  7.06s/it, loss_diff=1.473e-06, lr=0.1, val_loss=0.06558] \n",
      "[I 2023-12-08 12:05:19,241] Trial 10 finished with value: 0.06558003848338187 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:42<00:00,  7.42s/it, loss_diff=-8.817e-05, lr=0.1, val_loss=0.06497]\n",
      "[I 2023-12-08 12:09:08,277] Trial 11 finished with value: 0.06497492278383495 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch: 100%|██████████| 30/30 [03:36<00:00,  7.21s/it, loss_diff=-3.154e-05, lr=0.1, val_loss=0.06695]\n",
      "[I 2023-12-08 12:12:50,625] Trial 12 finished with value: 0.0657598661791096 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 2 with value: 0.0604385759999103.\n",
      "Epoch:  67%|██████▋   | 20/30 [02:27<01:13,  7.39s/it, loss_diff=-1.911e-05, lr=0.1, val_loss=0.06591]\n",
      "[W 2023-12-08 12:15:23,317] Trial 13 failed with parameters: {'l2_regularization_coeff': 0.0001} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 571, in <lambda>\n",
      "    lambda trial: self.optuna_objective(\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 556, in optuna_objective\n",
      "    losses = burrito.train(epochs=self.epochs)\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 384, in train\n",
      "    train_loss = self.process_data_loader(\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 331, in process_data_loader\n",
      "    loss = self._calculate_loss(encoded_parents, masks, mutation_indicators)\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 437, in _calculate_loss\n",
      "    mutation_indicator_masked = mutation_indicators[masks].float()\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-08 12:15:23,319] Trial 13 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsen/re/netam-experiments-1/human/cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optuna_steps \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m hyper_burrito \u001b[39m=\u001b[39m framework\u001b[39m.\u001b[39mHyperBurrito(pick_device(), train_dataset, val_dataset, models\u001b[39m.\u001b[39mCNNModel,  epochs\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m hyper_burrito\u001b[39m.\u001b[39;49moptuna_optimize(optuna_steps, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:570\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_optimize\u001b[0;34m(self, n_trials, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptuna_optimize\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     n_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m     fixed_hyperparams\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m ):\n\u001b[1;32m    569\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 570\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    571\u001b[0m         \u001b[39mlambda\u001b[39;49;00m trial: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptuna_objective(\n\u001b[1;32m    572\u001b[0m             trial,\n\u001b[1;32m    573\u001b[0m             int_params,\n\u001b[1;32m    574\u001b[0m             cat_params,\n\u001b[1;32m    575\u001b[0m             float_params,\n\u001b[1;32m    576\u001b[0m             log_float_params,\n\u001b[1;32m    577\u001b[0m             fixed_hyperparams,\n\u001b[1;32m    578\u001b[0m         ),\n\u001b[1;32m    579\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m     best_hyperparams \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    582\u001b[0m     best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:571\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_optimize.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptuna_optimize\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     n_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m     fixed_hyperparams\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m ):\n\u001b[1;32m    569\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    570\u001b[0m     study\u001b[39m.\u001b[39moptimize(\n\u001b[0;32m--> 571\u001b[0m         \u001b[39mlambda\u001b[39;00m trial: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptuna_objective(\n\u001b[1;32m    572\u001b[0m             trial,\n\u001b[1;32m    573\u001b[0m             int_params,\n\u001b[1;32m    574\u001b[0m             cat_params,\n\u001b[1;32m    575\u001b[0m             float_params,\n\u001b[1;32m    576\u001b[0m             log_float_params,\n\u001b[1;32m    577\u001b[0m             fixed_hyperparams,\n\u001b[1;32m    578\u001b[0m         ),\n\u001b[1;32m    579\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m     best_hyperparams \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    582\u001b[0m     best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:556\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_objective\u001b[0;34m(self, trial, int_params, cat_params, float_params, log_float_params, fixed_hyperparams)\u001b[0m\n\u001b[1;32m    553\u001b[0m burrito_hyperparams \u001b[39m=\u001b[39m filter_kwargs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mburrito_of_model, hyperparams)\n\u001b[1;32m    554\u001b[0m burrito \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mburrito_of_model(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mburrito_hyperparams)\n\u001b[0;32m--> 556\u001b[0m losses \u001b[39m=\u001b[39m burrito\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m    558\u001b[0m \u001b[39mreturn\u001b[39;00m losses[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmin()\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:384\u001b[0m, in \u001b[0;36mBurrito.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m    380\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStopping training early: learning rate below \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_learning_rate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    382\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_data_loader(\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, train_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    387\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_data_loader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loader, train_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(val_loss)\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:331\u001b[0m, in \u001b[0;36mBurrito.process_data_loader\u001b[0;34m(self, data_loader, train_mode)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(train_mode):\n\u001b[1;32m    330\u001b[0m     \u001b[39mfor\u001b[39;00m encoded_parents, masks, mutation_indicators \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m--> 331\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calculate_loss(encoded_parents, masks, mutation_indicators)\n\u001b[1;32m    333\u001b[0m         \u001b[39mif\u001b[39;00m train_mode:\n\u001b[1;32m    334\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mregularization_loss\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:437\u001b[0m, in \u001b[0;36mBurrito._calculate_loss\u001b[0;34m(self, encoded_parents, masks, mutation_indicators)\u001b[0m\n\u001b[1;32m    435\u001b[0m mut_prob \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mrates \u001b[39m*\u001b[39m mutation_freq)\n\u001b[1;32m    436\u001b[0m mut_prob_masked \u001b[39m=\u001b[39m mut_prob[masks]\n\u001b[0;32m--> 437\u001b[0m mutation_indicator_masked \u001b[39m=\u001b[39m mutation_indicators[masks]\u001b[39m.\u001b[39;49mfloat()\n\u001b[1;32m    438\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbce_loss(mut_prob_masked, mutation_indicator_masked)\n\u001b[1;32m    439\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [I 2023-12-03 13:38:46,435] Trial 37 finished with value: 0.0603752824113363 and parameters: {'kernel_size': 19, 'dropout': 0.0, 'embedding_dim': 12, 'filter_count': 12}. Best is trial 37 with value: 0.0603752824113363.\n",
    "\n",
    "cat_params = {\n",
    "    \"l2_regularization_coeff\": [1e-6, 1e-5, 1e-4],\n",
    "}\n",
    "int_params = {\n",
    "}\n",
    "float_params = {\n",
    "}\n",
    "log_float_params = {\n",
    "}\n",
    "# Note that if anything appears below and above, the above gets priority.\n",
    "fixed_hyperparams = {\n",
    "    \"kmer_length\": kmer_length,\n",
    "    \"embedding_dim\": 14,\n",
    "    \"filter_count\": 25,\n",
    "    \"kernel_size\": 15,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"min_learning_rate\": 1e-3, # early stopping!\n",
    "    \"l2_regularization_coeff\": 1e-6,\n",
    "    \"min_parameter_count\": 1000,\n",
    "    \"max_parameter_count\": 10000,\n",
    "}\n",
    "epochs = 30\n",
    "optuna_steps = 50\n",
    "\n",
    "hyper_burrito = framework.HyperBurrito(pick_device(), train_dataset, val_dataset, models.CNNModel,  epochs=epochs)\n",
    "\n",
    "hyper_burrito.optuna_optimize(optuna_steps, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with no dropout:\n",
    "[I 2023-12-08 10:42:00,624] Trial 1 finished with value: 0.0604963169044101 and parameters: {'l2_regularization_coeff': 0.0001}. Best is trial 1 with value: 0.0604963169044101.\n",
    "\n",
    "but this didn't consistently work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
