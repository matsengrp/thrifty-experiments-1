{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from netam import framework, models\n",
    "from netam.common import pick_device, print_parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shmoof_data_path = \"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\"\n",
    "shmoof_data_path = \"/Users/matsen/data/shmoof_pcp_2023-11-30_MASKED.csv\"\n",
    "train_df, val_df = framework.load_shmoof_dataframes(shmoof_data_path, val_nickname=\"51\") # , sample_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "we have 26592 training examples and 22424 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 3\n",
    "site_count = 500\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, site_count=site_count)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, site_count=site_count)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Best Hyperparameters: {'kernel_size': 11, 'dropout': 0.3, 'embedding_dim': 7, 'num_filters': 19}\n",
    "model = models.CNNModel(train_dataset, embedding_dim=7, num_filters=19, kernel_size=11, dropout_rate=0.3)\n",
    "print_parameter_count(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "burrito = framework.Burrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-5, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)\n",
    "torch.save(model, \"_ignore/cnn_lrg.pt\")\n",
    "losses.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 15:29:46,715] A new study created in memory with name: no-name-3a88dc25-8f75-454a-9cd6-c8925d38491a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 54/1000 [06:12<1:48:37,  6.89s/it, loss_diff=7.577e-07, lr=3.2e-5, val_loss=0.06045]  \n",
      "[I 2023-12-03 15:36:04,476] Trial 0 finished with value: 0.06043745133702856 and parameters: {'dropout': 0.2}. Best is trial 0 with value: 0.06043745133702856.\n",
      "Epoch:   5%|▍         | 47/1000 [06:00<2:01:55,  7.68s/it, loss_diff=1.709e-06, lr=3.2e-5, val_loss=0.06048]  \n",
      "[I 2023-12-03 15:42:11,047] Trial 1 finished with value: 0.060472374750733845 and parameters: {'dropout': 0.3}. Best is trial 0 with value: 0.06043745133702856.\n",
      "Epoch:   5%|▌         | 54/1000 [06:41<1:57:17,  7.44s/it, loss_diff=1.483e-06, lr=3.2e-5, val_loss=0.06046]  \n",
      "[I 2023-12-03 15:48:57,720] Trial 2 finished with value: 0.060418437370286855 and parameters: {'dropout': 0.0}. Best is trial 2 with value: 0.060418437370286855.\n",
      "Epoch:  18%|█▊        | 177/1000 [22:42<1:45:36,  7.70s/it, loss_diff=3.567e-08, lr=3.2e-5, val_loss=0.06152]  \n",
      "[I 2023-12-03 16:11:46,741] Trial 3 finished with value: 0.06150473749827922 and parameters: {'dropout': 0.3}. Best is trial 2 with value: 0.060418437370286855.\n",
      "Epoch:   4%|▍         | 40/1000 [04:58<1:59:22,  7.46s/it, loss_diff=1.498e-07, lr=3.2e-5, val_loss=0.06039]  \n",
      "[I 2023-12-03 16:16:51,124] Trial 4 finished with value: 0.06038426336324985 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:  30%|███       | 303/1000 [39:56<1:31:52,  7.91s/it, loss_diff=8.604e-08, lr=3.2e-5, val_loss=0.06073]  \n",
      "[I 2023-12-03 16:56:53,725] Trial 5 finished with value: 0.06072770213277009 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:  24%|██▎       | 235/1000 [30:32<1:39:25,  7.80s/it, loss_diff=-2.254e-07, lr=3.2e-5, val_loss=0.06113] \n",
      "[I 2023-12-03 17:27:32,908] Trial 6 finished with value: 0.0611316313381668 and parameters: {'dropout': 0.3}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   6%|▌         | 56/1000 [07:45<2:10:47,  8.31s/it, loss_diff=8.726e-07, lr=3.2e-5, val_loss=0.06063]  \n",
      "[I 2023-12-03 17:35:24,859] Trial 7 finished with value: 0.06063095584134311 and parameters: {'dropout': 0.3}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   5%|▌         | 50/1000 [06:30<2:03:36,  7.81s/it, loss_diff=-5.095e-07, lr=3.2e-5, val_loss=0.06077] \n",
      "[I 2023-12-03 17:42:01,444] Trial 8 finished with value: 0.0607742300357429 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   4%|▍         | 42/1000 [05:43<2:10:34,  8.18s/it, loss_diff=-1.538e-06, lr=3.2e-5, val_loss=0.06043] \n",
      "[I 2023-12-03 17:47:51,671] Trial 9 finished with value: 0.06042273131292546 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   8%|▊         | 83/1000 [10:52<2:00:08,  7.86s/it, loss_diff=1.804e-07, lr=3.2e-5, val_loss=0.06098]  \n",
      "[I 2023-12-03 17:58:50,873] Trial 10 finished with value: 0.06097591439108529 and parameters: {'dropout': 0.1}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:  10%|▉         | 95/1000 [12:51<2:02:29,  8.12s/it, loss_diff=-1.468e-06, lr=3.2e-5, val_loss=0.06125] \n",
      "[I 2023-12-03 18:11:48,977] Trial 11 finished with value: 0.06125016728315743 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   6%|▌         | 57/1000 [07:37<2:06:10,  8.03s/it, loss_diff=-4.109e-07, lr=3.2e-5, val_loss=0.06048] \n",
      "[I 2023-12-03 18:19:33,005] Trial 12 finished with value: 0.06047704254346493 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   6%|▌         | 60/1000 [07:00<1:49:54,  7.02s/it, loss_diff=-7.686e-08, lr=3.2e-5, val_loss=0.06555] \n",
      "[I 2023-12-03 18:26:40,733] Trial 13 finished with value: 0.0655452146663693 and parameters: {'dropout': 0.2}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   4%|▍         | 44/1000 [05:49<2:06:36,  7.95s/it, loss_diff=8.382e-08, lr=3.2e-5, val_loss=0.06047]  \n",
      "[I 2023-12-03 18:32:37,084] Trial 14 finished with value: 0.06043625519057065 and parameters: {'dropout': 0.1}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   4%|▍         | 42/1000 [05:56<2:15:26,  8.48s/it, loss_diff=6.226e-07, lr=3.2e-5, val_loss=0.06042]  \n",
      "[I 2023-12-03 18:38:40,525] Trial 15 finished with value: 0.0604185373577164 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   8%|▊         | 83/1000 [11:08<2:03:03,  8.05s/it, loss_diff=-1.76e-06, lr=3.2e-5, val_loss=0.06122]  \n",
      "[I 2023-12-03 18:49:56,683] Trial 16 finished with value: 0.06121789483945581 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   5%|▍         | 47/1000 [06:43<2:16:21,  8.59s/it, loss_diff=-3.485e-07, lr=3.2e-5, val_loss=0.06047] \n",
      "[I 2023-12-03 18:56:47,542] Trial 17 finished with value: 0.060458292636410324 and parameters: {'dropout': 0.0}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:  16%|█▌        | 161/1000 [20:49<1:48:29,  7.76s/it, loss_diff=-5.554e-08, lr=3.2e-5, val_loss=0.06208] \n",
      "[I 2023-12-03 19:17:44,245] Trial 18 finished with value: 0.06207383491824535 and parameters: {'dropout': 0.1}. Best is trial 4 with value: 0.06038426336324985.\n",
      "Epoch:   3%|▎         | 31/1000 [03:23<1:46:14,  6.58s/it, loss_diff=-5.104e-05, lr=0.1, val_loss=0.06703]\n",
      "[W 2023-12-03 19:21:12,707] Trial 19 failed with parameters: {'dropout': 0.2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 552, in <lambda>\n",
      "    lambda trial: self.optuna_objective(\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 537, in optuna_objective\n",
      "    losses = burrito.train(epochs=self.epochs)\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 380, in train\n",
      "    train_loss = self.process_data_loader(\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 326, in process_data_loader\n",
      "    for encoded_parents, masks, mutation_indicators in data_loader:\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-03 19:21:12,710] Trial 19 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsen/re/netam-experiments-1/human/cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optuna_steps \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m hyper_burrito \u001b[39m=\u001b[39m framework\u001b[39m.\u001b[39mHyperBurrito(pick_device(), train_dataset, val_dataset, models\u001b[39m.\u001b[39mCNNModel,  epochs\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsen/re/netam-experiments-1/human/cnn.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m hyper_burrito\u001b[39m.\u001b[39;49moptuna_optimize(optuna_steps, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:551\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_optimize\u001b[0;34m(self, n_trials, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptuna_optimize\u001b[39m(\n\u001b[1;32m    542\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    543\u001b[0m     n_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m     fixed_hyperparams\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m ):\n\u001b[1;32m    550\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    552\u001b[0m         \u001b[39mlambda\u001b[39;49;00m trial: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptuna_objective(\n\u001b[1;32m    553\u001b[0m             trial,\n\u001b[1;32m    554\u001b[0m             int_params,\n\u001b[1;32m    555\u001b[0m             cat_params,\n\u001b[1;32m    556\u001b[0m             float_params,\n\u001b[1;32m    557\u001b[0m             log_float_params,\n\u001b[1;32m    558\u001b[0m             fixed_hyperparams,\n\u001b[1;32m    559\u001b[0m         ),\n\u001b[1;32m    560\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m     best_hyperparams \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    563\u001b[0m     best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:552\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_optimize.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptuna_optimize\u001b[39m(\n\u001b[1;32m    542\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    543\u001b[0m     n_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m     fixed_hyperparams\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m ):\n\u001b[1;32m    550\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     study\u001b[39m.\u001b[39moptimize(\n\u001b[0;32m--> 552\u001b[0m         \u001b[39mlambda\u001b[39;00m trial: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptuna_objective(\n\u001b[1;32m    553\u001b[0m             trial,\n\u001b[1;32m    554\u001b[0m             int_params,\n\u001b[1;32m    555\u001b[0m             cat_params,\n\u001b[1;32m    556\u001b[0m             float_params,\n\u001b[1;32m    557\u001b[0m             log_float_params,\n\u001b[1;32m    558\u001b[0m             fixed_hyperparams,\n\u001b[1;32m    559\u001b[0m         ),\n\u001b[1;32m    560\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m     best_hyperparams \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    563\u001b[0m     best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:537\u001b[0m, in \u001b[0;36mHyperBurrito.optuna_objective\u001b[0;34m(self, trial, int_params, cat_params, float_params, log_float_params, fixed_hyperparams)\u001b[0m\n\u001b[1;32m    534\u001b[0m burrito_hyperparams \u001b[39m=\u001b[39m filter_kwargs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mburrito_of_model, hyperparams)\n\u001b[1;32m    535\u001b[0m burrito \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mburrito_of_model(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mburrito_hyperparams)\n\u001b[0;32m--> 537\u001b[0m losses \u001b[39m=\u001b[39m burrito\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m    539\u001b[0m \u001b[39mreturn\u001b[39;00m losses[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmin()\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:380\u001b[0m, in \u001b[0;36mBurrito.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m    376\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStopping training early: learning rate below \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_learning_rate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         )\n\u001b[1;32m    378\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_data_loader(\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, train_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    382\u001b[0m )\n\u001b[1;32m    383\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_data_loader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loader, train_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(val_loss)\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:326\u001b[0m, in \u001b[0;36mBurrito.process_data_loader\u001b[0;34m(self, data_loader, train_mode)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    325\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(train_mode):\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mfor\u001b[39;00m encoded_parents, masks, mutation_indicators \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m    327\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_loss(encoded_parents, masks, mutation_indicators)\n\u001b[1;32m    329\u001b[0m         \u001b[39mif\u001b[39;00m train_mode:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [I 2023-12-03 13:38:46,435] Trial 37 finished with value: 0.0603752824113363 and parameters: {'kernel_size': 19, 'dropout': 0.0, 'embedding_dim': 12, 'filter_count': 12}. Best is trial 37 with value: 0.0603752824113363.\n",
    "\n",
    "cat_params = {\n",
    "    \"dropout\": [0.0, 0.1, 0.2, 0.3],\n",
    "}\n",
    "int_params = {\n",
    "}\n",
    "float_params = {\n",
    "}\n",
    "log_float_params = {\n",
    "}\n",
    "# Note that if anything appears below and above, the above gets priority.\n",
    "fixed_hyperparams = {\n",
    "    \"filter_count\": 12,\n",
    "    \"embedding_dim\": 12,\n",
    "    \"kernel_size\": 19,\n",
    "    \"kmer_length\": kmer_length,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"min_learning_rate\": 1e-3, # early stopping!\n",
    "    \"dropout\": 0.1,\n",
    "    \"l2_regularization_coeff\": 1e-6,\n",
    "    \"min_parameter_count\": 2048,\n",
    "    \"max_parameter_count\": 4096,\n",
    "}\n",
    "epochs = 1000\n",
    "optuna_steps = 50\n",
    "\n",
    "hyper_burrito = framework.HyperBurrito(pick_device(), train_dataset, val_dataset, models.CNNModel,  epochs=epochs)\n",
    "\n",
    "hyper_burrito.optuna_optimize(optuna_steps, cat_params, int_params, float_params, log_float_params, fixed_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
