{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNMLPModel\n",
    "\n",
    "Here I test if we do better by inserting a hidden layer in the CNN model.\n",
    "The answer is no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from netam import framework, models\n",
    "from netam.framework import calculate_loss\n",
    "from epam.torch_common import pick_device\n",
    "\n",
    "from epam.torch_common import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shmoof_data_path = \"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\"\n",
    "train_df, val_df = framework.load_shmoof_dataframes(shmoof_data_path, val_nickname=\"59\") #, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "we have 44330 training examples and 4686 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 3\n",
    "max_length = 410\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMLPModel(\n",
       "  (kmer_embedding): Embedding(65, 10)\n",
       "  (conv): Conv1d(10, 9, kernel_size=(11,), stride=(1,), padding=same)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_linear): Linear(in_features=9, out_features=5, bias=True)\n",
       "  (output_linear): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNNMLPModel(nn.Module):\n",
    "    def __init__(self, dataset, embedding_dim, num_filters, kernel_size, hidden_dim, dropout_rate=0.1):\n",
    "        super(CNNMLPModel, self).__init__()\n",
    "        self.kmer_count = len(dataset.kmer_to_index)\n",
    "        self.kmer_embedding = nn.Embedding(self.kmer_count, embedding_dim)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size, padding='same')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.hidden_linear = nn.Linear(in_features=num_filters, out_features=hidden_dim)\n",
    "        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, encoded_parents, masks):\n",
    "        kmer_embeds = self.kmer_embedding(encoded_parents)\n",
    "        kmer_embeds = kmer_embeds.permute(0, 2, 1)  # Transpose for Conv1D\n",
    "        conv_out = F.relu(self.conv(kmer_embeds))\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # Transpose back for Linear layer\n",
    "        hidden_out = F.relu(self.hidden_linear(conv_out))\n",
    "        log_rates = self.output_linear(hidden_out).squeeze(-1)\n",
    "        rates = torch.exp(log_rates * masks)\n",
    "\n",
    "        return rates\n",
    "\n",
    "\n",
    "\n",
    "model = CNNMLPModel(train_dataset, embedding_dim=10, num_filters=9, kernel_size=11, hidden_dim=5, dropout_rate=0.1)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  37%|███▋      | 37/100 [05:44<09:46,  9.31s/it, loss_diff=-1.351e-06, lr=3.2e-5] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065654</td>\n",
       "      <td>0.061373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.058455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061467</td>\n",
       "      <td>0.058362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061244</td>\n",
       "      <td>0.058237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061101</td>\n",
       "      <td>0.058254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.061080</td>\n",
       "      <td>0.058290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.061052</td>\n",
       "      <td>0.058312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.061026</td>\n",
       "      <td>0.058234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.058239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.060840</td>\n",
       "      <td>0.058160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.058173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.060701</td>\n",
       "      <td>0.058158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.060709</td>\n",
       "      <td>0.058221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.060710</td>\n",
       "      <td>0.058194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.060712</td>\n",
       "      <td>0.058137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.058221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.060707</td>\n",
       "      <td>0.058165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.058122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.060703</td>\n",
       "      <td>0.058201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.060679</td>\n",
       "      <td>0.058247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.058186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.060687</td>\n",
       "      <td>0.058148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.058172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.058125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.060565</td>\n",
       "      <td>0.058129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.060551</td>\n",
       "      <td>0.058119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.060528</td>\n",
       "      <td>0.058119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.058142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.060523</td>\n",
       "      <td>0.058135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.060507</td>\n",
       "      <td>0.058144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.060501</td>\n",
       "      <td>0.058132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.060506</td>\n",
       "      <td>0.058143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.058151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.060502</td>\n",
       "      <td>0.058149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.060493</td>\n",
       "      <td>0.058146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.058145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.058145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.060502</td>\n",
       "      <td>0.058144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss\n",
       "0     0.065654  0.061373\n",
       "1     0.063354  0.058455\n",
       "2     0.061467  0.058362\n",
       "3     0.061244  0.058237\n",
       "4     0.061101  0.058254\n",
       "5     0.061080  0.058290\n",
       "6     0.061052  0.058312\n",
       "7     0.061026  0.058234\n",
       "8     0.061034  0.058239\n",
       "9     0.060840  0.058160\n",
       "10    0.060741  0.058173\n",
       "11    0.060701  0.058158\n",
       "12    0.060709  0.058221\n",
       "13    0.060710  0.058194\n",
       "14    0.060712  0.058137\n",
       "15    0.060700  0.058221\n",
       "16    0.060707  0.058165\n",
       "17    0.060700  0.058122\n",
       "18    0.060703  0.058201\n",
       "19    0.060679  0.058247\n",
       "20    0.060692  0.058186\n",
       "21    0.060687  0.058148\n",
       "22    0.060692  0.058172\n",
       "23    0.060602  0.058125\n",
       "24    0.060565  0.058129\n",
       "25    0.060551  0.058119\n",
       "26    0.060528  0.058119\n",
       "27    0.060547  0.058142\n",
       "28    0.060523  0.058135\n",
       "29    0.060507  0.058144\n",
       "30    0.060501  0.058132\n",
       "31    0.060506  0.058143\n",
       "32    0.060503  0.058151\n",
       "33    0.060502  0.058149\n",
       "34    0.060493  0.058146\n",
       "35    0.060495  0.058145\n",
       "36    0.060503  0.058145\n",
       "37    0.060502  0.058144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burrito = framework.Burrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-4, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here's a simple CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  34%|███▍      | 34/100 [04:57<09:38,  8.76s/it, loss_diff=7.136e-07, lr=3.2e-5]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065707</td>\n",
       "      <td>0.061421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062443</td>\n",
       "      <td>0.058267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061315</td>\n",
       "      <td>0.058295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061204</td>\n",
       "      <td>0.058136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061114</td>\n",
       "      <td>0.058215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.061115</td>\n",
       "      <td>0.058206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.058082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.061072</td>\n",
       "      <td>0.058211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.061078</td>\n",
       "      <td>0.058248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.061053</td>\n",
       "      <td>0.058174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.061030</td>\n",
       "      <td>0.058147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.061055</td>\n",
       "      <td>0.058108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.060843</td>\n",
       "      <td>0.057985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.060752</td>\n",
       "      <td>0.058001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.058011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.060750</td>\n",
       "      <td>0.058031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.060755</td>\n",
       "      <td>0.058006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.058032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.060680</td>\n",
       "      <td>0.057989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.060657</td>\n",
       "      <td>0.057971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.060652</td>\n",
       "      <td>0.057988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.060649</td>\n",
       "      <td>0.057993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.060640</td>\n",
       "      <td>0.057987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.060650</td>\n",
       "      <td>0.057989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.060643</td>\n",
       "      <td>0.058003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.060610</td>\n",
       "      <td>0.057988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.060616</td>\n",
       "      <td>0.057986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.057985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.060622</td>\n",
       "      <td>0.057986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.060622</td>\n",
       "      <td>0.057981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.057981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.057982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.060607</td>\n",
       "      <td>0.057982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.060619</td>\n",
       "      <td>0.057982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.057983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss\n",
       "0     0.065707  0.061421\n",
       "1     0.062443  0.058267\n",
       "2     0.061315  0.058295\n",
       "3     0.061204  0.058136\n",
       "4     0.061114  0.058215\n",
       "5     0.061115  0.058206\n",
       "6     0.061119  0.058082\n",
       "7     0.061072  0.058211\n",
       "8     0.061078  0.058248\n",
       "9     0.061053  0.058174\n",
       "10    0.061030  0.058147\n",
       "11    0.061055  0.058108\n",
       "12    0.060843  0.057985\n",
       "13    0.060752  0.058001\n",
       "14    0.060754  0.058011\n",
       "15    0.060750  0.058031\n",
       "16    0.060755  0.058006\n",
       "17    0.060753  0.058032\n",
       "18    0.060680  0.057989\n",
       "19    0.060657  0.057971\n",
       "20    0.060652  0.057988\n",
       "21    0.060649  0.057993\n",
       "22    0.060640  0.057987\n",
       "23    0.060650  0.057989\n",
       "24    0.060643  0.058003\n",
       "25    0.060610  0.057988\n",
       "26    0.060616  0.057986\n",
       "27    0.060618  0.057985\n",
       "28    0.060622  0.057986\n",
       "29    0.060622  0.057981\n",
       "30    0.060606  0.057981\n",
       "31    0.060606  0.057982\n",
       "32    0.060607  0.057982\n",
       "33    0.060619  0.057982\n",
       "34    0.060602  0.057983"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.CNNModel(train_dataset, embedding_dim=10, num_filters=9, kernel_size=11, dropout_rate=0.1)\n",
    "model.to(device)\n",
    "\n",
    "burrito = framework.Burrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-4, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)\n",
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
