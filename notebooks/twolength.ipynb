{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from netam import framework, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = framework.load_shmoof_dataframes(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\")# , sample_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tang data\n",
    "\n",
    "# full_df = pd.read_csv(\"/Users/matsen/data/tang-deepshm_size2_edges_22-May-2023.branch_length.csv\", index_col=0).reset_index(drop=True)\n",
    "# \n",
    "# # only keep rows where parent is different than child\n",
    "# full_df = full_df[full_df[\"parent\"] != full_df[\"child\"]]\n",
    "# \n",
    "# train_df = full_df.sample(frac=0.8)\n",
    "# val_df = full_df.drop(train_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 35830 training examples and 13186 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 5\n",
    "max_length = 410\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35830, 410])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"parent\"].str.len().median()\n",
    "#train_df[\"v_int_end\"].hist(bins=100)\n",
    "# get the index of the maximum True entry of train_dataset[0][1]\n",
    "masks = train_dataset.masks\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(364)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_length_of_masks(masks):\n",
    "    batch_size, seq_length = masks.shape\n",
    "\n",
    "    range_tensor = torch.arange(seq_length).repeat(batch_size, 1)\n",
    "    masked_range = torch.where(masks, range_tensor, torch.tensor(-1))\n",
    "\n",
    "    return masked_range.max(dim=1)[0]\n",
    "\n",
    "sequence_length_of_masks(masks).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLengthSHMoofModel(nn.Module):\n",
    "    def __init__(self, dataset, length_threshold, residual_penalty_weight=0.0):\n",
    "        super(TwoLengthSHMoofModel, self).__init__()\n",
    "        self.kmer_count = len(dataset.kmer_to_index)\n",
    "        self.site_count = dataset.max_length\n",
    "        self.length_threshold = length_threshold\n",
    "        self.residual_penalty_weight = residual_penalty_weight\n",
    "\n",
    "        self.kmer_embedding = nn.Embedding(self.kmer_count, 1)\n",
    "        self.log_site_rates = nn.Embedding(self.site_count, 1)\n",
    "        self.log_site_rates_long_residual = nn.Embedding(self.site_count, 1)\n",
    "\n",
    "    def forward(self, encoded_parents, masks):\n",
    "        log_kmer_rates = self.kmer_embedding(encoded_parents).squeeze()\n",
    "        sequence_length = sequence_length_of_masks(masks)\n",
    "\n",
    "        # Determine if the sequence is long or short\n",
    "        is_long = sequence_length > self.length_threshold\n",
    "        is_long = is_long.unsqueeze(-1)\n",
    "\n",
    "        log_site_rates_short = self.log_site_rates.weight.T.expand_as(log_kmer_rates)\n",
    "        # set log_site_rates_short to zero for all indices beyond the length threshold\n",
    "        log_site_rates_short = torch.where(is_long, torch.tensor(0.0), log_site_rates_short)\n",
    "        log_site_rates_long_residual = self.log_site_rates_long_residual.weight.T.expand_as(log_kmer_rates)\n",
    "        \n",
    "        # Adjust log_site_rates for long sequences\n",
    "        log_site_rates_long = log_site_rates_short + log_site_rates_long_residual\n",
    "        log_site_rates = torch.where(is_long, log_site_rates_long, log_site_rates_short)\n",
    "\n",
    "        rates = torch.exp(log_kmer_rates + log_site_rates)\n",
    "        return rates\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        # Calculate L2 norm (squared sum) of the log_site_rates_long_residual weights\n",
    "        reg_loss = torch.sum(self.log_site_rates_long_residual.weight ** 2)\n",
    "        # Apply the regularization weight\n",
    "        reg_loss *= self.residual_penalty_weight\n",
    "        return reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [1/100]\t Loss: 0.066157887\t Val Loss: 0.067732337\n",
      "Epoch [2/100]\t Loss: 0.059485697\t Val Loss: 0.067316885\n",
      "Epoch [3/100]\t Loss: 0.059339037\t Val Loss: 0.067260014\n",
      "Epoch [4/100]\t Loss: 0.059325151\t Val Loss: 0.067260171\n",
      "Epoch [5/100]\t Loss: 0.059315414\t Val Loss: 0.067251524\n",
      "Epoch [6/100]\t Loss: 0.059302538\t Val Loss: 0.067225636\n",
      "Epoch [7/100]\t Loss: 0.059294048\t Val Loss: 0.067249244\n",
      "Epoch [8/100]\t Loss: 0.059288405\t Val Loss: 0.067247988\n",
      "Epoch [9/100]\t Loss: 0.059276572\t Val Loss: 0.067150525\n",
      "Epoch [10/100]\t Loss: 0.059265354\t Val Loss: 0.067193323\n",
      "Epoch [11/100]\t Loss: 0.059248385\t Val Loss: 0.067125562\n",
      "Epoch [12/100]\t Loss: 0.059231946\t Val Loss: 0.067146557\n",
      "Epoch [13/100]\t Loss: 0.059227465\t Val Loss: 0.067140751\n",
      "Epoch [14/100]\t Loss: 0.059211675\t Val Loss: 0.067143321\n",
      "Epoch [15/100]\t Loss: 0.059214812\t Val Loss: 0.067136307\n",
      "Epoch [16/100]\t Loss: 0.059189886\t Val Loss: 0.067144968\n",
      "Epoch 00016: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch [17/100]\t Loss: 0.059048955\t Val Loss: 0.066970541\n",
      "Epoch [18/100]\t Loss: 0.058979308\t Val Loss: 0.066953787\n",
      "Epoch [19/100]\t Loss: 0.058975682\t Val Loss: 0.066947146\n",
      "Epoch [20/100]\t Loss: 0.058973362\t Val Loss: 0.066947192\n",
      "Epoch [21/100]\t Loss: 0.058968968\t Val Loss: 0.066941296\n",
      "Epoch [22/100]\t Loss: 0.058967348\t Val Loss: 0.06694259\n",
      "Epoch [23/100]\t Loss: 0.0589634\t Val Loss: 0.06693676\n",
      "Epoch [24/100]\t Loss: 0.058961873\t Val Loss: 0.066924574\n",
      "Epoch [25/100]\t Loss: 0.058958033\t Val Loss: 0.066931412\n",
      "Epoch [26/100]\t Loss: 0.05895388\t Val Loss: 0.066929942\n",
      "Epoch [27/100]\t Loss: 0.058953021\t Val Loss: 0.066924481\n",
      "Epoch [28/100]\t Loss: 0.058950611\t Val Loss: 0.066929296\n",
      "Epoch [29/100]\t Loss: 0.058944606\t Val Loss: 0.066913688\n",
      "Epoch [30/100]\t Loss: 0.058943726\t Val Loss: 0.066912626\n",
      "Epoch [31/100]\t Loss: 0.058941633\t Val Loss: 0.066906527\n",
      "Epoch [32/100]\t Loss: 0.058937132\t Val Loss: 0.066915683\n",
      "Epoch [33/100]\t Loss: 0.058931882\t Val Loss: 0.06690452\n",
      "Epoch [34/100]\t Loss: 0.05892498\t Val Loss: 0.06690031\n",
      "Epoch [35/100]\t Loss: 0.058925653\t Val Loss: 0.066903475\n",
      "Epoch [36/100]\t Loss: 0.058920471\t Val Loss: 0.066897324\n",
      "Epoch [37/100]\t Loss: 0.058920302\t Val Loss: 0.06689015\n",
      "Epoch [38/100]\t Loss: 0.058914293\t Val Loss: 0.066880136\n",
      "Epoch [39/100]\t Loss: 0.058912047\t Val Loss: 0.066881287\n",
      "Epoch [40/100]\t Loss: 0.058908935\t Val Loss: 0.066872377\n",
      "Epoch [41/100]\t Loss: 0.058903102\t Val Loss: 0.066867952\n",
      "Epoch [42/100]\t Loss: 0.058901276\t Val Loss: 0.066870708\n",
      "Epoch [43/100]\t Loss: 0.05889719\t Val Loss: 0.06687155\n",
      "Epoch [44/100]\t Loss: 0.058893691\t Val Loss: 0.066863107\n",
      "Epoch [45/100]\t Loss: 0.05888469\t Val Loss: 0.066859823\n",
      "Epoch [46/100]\t Loss: 0.058881778\t Val Loss: 0.066855437\n",
      "Epoch [47/100]\t Loss: 0.058880844\t Val Loss: 0.066851173\n",
      "Epoch [48/100]\t Loss: 0.058874835\t Val Loss: 0.066853234\n",
      "Epoch [49/100]\t Loss: 0.058868284\t Val Loss: 0.066844429\n",
      "Epoch [50/100]\t Loss: 0.058866162\t Val Loss: 0.06683709\n",
      "Epoch [51/100]\t Loss: 0.058859671\t Val Loss: 0.066847352\n",
      "Epoch [52/100]\t Loss: 0.058859566\t Val Loss: 0.066824552\n",
      "Epoch [53/100]\t Loss: 0.058852491\t Val Loss: 0.066830293\n",
      "Epoch [54/100]\t Loss: 0.058846695\t Val Loss: 0.066822534\n",
      "Epoch [55/100]\t Loss: 0.058843336\t Val Loss: 0.066815425\n",
      "Epoch [56/100]\t Loss: 0.058843178\t Val Loss: 0.066818036\n",
      "Epoch [57/100]\t Loss: 0.058839713\t Val Loss: 0.066812445\n",
      "Epoch [58/100]\t Loss: 0.058832552\t Val Loss: 0.066811976\n",
      "Epoch [59/100]\t Loss: 0.058828759\t Val Loss: 0.066791606\n",
      "Epoch [60/100]\t Loss: 0.058827071\t Val Loss: 0.06680161\n",
      "Epoch [61/100]\t Loss: 0.058822348\t Val Loss: 0.066793313\n",
      "Epoch [62/100]\t Loss: 0.05881542\t Val Loss: 0.066790356\n",
      "Epoch [63/100]\t Loss: 0.058813876\t Val Loss: 0.066784503\n",
      "Epoch [64/100]\t Loss: 0.058805973\t Val Loss: 0.066787864\n",
      "Epoch [65/100]\t Loss: 0.058805355\t Val Loss: 0.066774722\n",
      "Epoch [66/100]\t Loss: 0.058800197\t Val Loss: 0.066771567\n",
      "Epoch [67/100]\t Loss: 0.058794893\t Val Loss: 0.066778147\n",
      "Epoch [68/100]\t Loss: 0.058789934\t Val Loss: 0.066772319\n",
      "Epoch [69/100]\t Loss: 0.058789143\t Val Loss: 0.066765407\n",
      "Epoch [70/100]\t Loss: 0.058785004\t Val Loss: 0.066769996\n",
      "Epoch [71/100]\t Loss: 0.058780003\t Val Loss: 0.066767302\n",
      "Epoch [72/100]\t Loss: 0.058774104\t Val Loss: 0.066756085\n",
      "Epoch [73/100]\t Loss: 0.05877002\t Val Loss: 0.066747811\n",
      "Epoch [74/100]\t Loss: 0.058766295\t Val Loss: 0.066754502\n",
      "Epoch [75/100]\t Loss: 0.05876533\t Val Loss: 0.066737165\n",
      "Epoch [76/100]\t Loss: 0.058755657\t Val Loss: 0.066740258\n",
      "Epoch [77/100]\t Loss: 0.058755142\t Val Loss: 0.066727964\n",
      "Epoch [78/100]\t Loss: 0.058749681\t Val Loss: 0.0667303\n",
      "Epoch [79/100]\t Loss: 0.0587463\t Val Loss: 0.06672699\n",
      "Epoch [80/100]\t Loss: 0.058742511\t Val Loss: 0.066723989\n",
      "Epoch [81/100]\t Loss: 0.058737466\t Val Loss: 0.06670715\n",
      "Epoch [82/100]\t Loss: 0.058731871\t Val Loss: 0.066709499\n",
      "Epoch [83/100]\t Loss: 0.058730048\t Val Loss: 0.066712859\n",
      "Epoch [84/100]\t Loss: 0.058722817\t Val Loss: 0.066717281\n",
      "Epoch [85/100]\t Loss: 0.058719805\t Val Loss: 0.066699885\n",
      "Epoch [86/100]\t Loss: 0.058716546\t Val Loss: 0.066707015\n",
      "Epoch [87/100]\t Loss: 0.058711225\t Val Loss: 0.066690685\n",
      "Epoch [88/100]\t Loss: 0.058710009\t Val Loss: 0.06668669\n",
      "Epoch [89/100]\t Loss: 0.05870642\t Val Loss: 0.066687052\n",
      "Epoch [90/100]\t Loss: 0.058700524\t Val Loss: 0.066676848\n",
      "Epoch [91/100]\t Loss: 0.058695633\t Val Loss: 0.066687808\n",
      "Epoch [92/100]\t Loss: 0.058690146\t Val Loss: 0.066676913\n",
      "Epoch [93/100]\t Loss: 0.058697716\t Val Loss: 0.066673705\n",
      "Epoch [94/100]\t Loss: 0.058687352\t Val Loss: 0.066666978\n",
      "Epoch [95/100]\t Loss: 0.058683617\t Val Loss: 0.066663876\n",
      "Epoch [96/100]\t Loss: 0.058679824\t Val Loss: 0.066669508\n",
      "Epoch [97/100]\t Loss: 0.05867354\t Val Loss: 0.06665545\n",
      "Epoch [98/100]\t Loss: 0.058673032\t Val Loss: 0.066662291\n",
      "Epoch [99/100]\t Loss: 0.058667338\t Val Loss: 0.066653953\n",
      "Epoch [100/100]\t Loss: 0.058666308\t Val Loss: 0.066652492\n"
     ]
    }
   ],
   "source": [
    "length_threshold = sequence_length_of_masks(masks).median()\n",
    "model = TwoLengthSHMoofModel(train_dataset, length_threshold=length_threshold, residual_penalty_weight=1e-5)\n",
    "burrito = framework.Burrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load motif mutabilities\n",
    "shmoof_motifs = pd.read_csv('_ignore/original_shmoof/mutabilities_context.tsv', sep='\\t')\n",
    "# rename Mutability column to Mutability_shmoof\n",
    "#shmoof_motifs = shmoof_motifs.rename(columns={'Mutability': 'Mutability_shmoof'})\n",
    "reshmoof_mutabilities = torch.exp(model.kmer_embedding.weight).squeeze().detach().numpy()\n",
    "reshmoof_motifs = pd.DataFrame({'Mutability': reshmoof_mutabilities, 'Motif': train_dataset.kmer_to_index.keys()})\n",
    "\n",
    "# Merge dataframes\n",
    "merged_motifs = pd.merge(shmoof_motifs, reshmoof_motifs, on='Motif', how='inner', suffixes=('_shmoof', '_reshmoof'))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(merged_motifs['Mutability_shmoof'], merged_motifs['Mutability_reshmoof'], alpha=0.5)\n",
    "\n",
    "# Determine bounds for y=x line\n",
    "min_bound = min(merged_motifs['Mutability_shmoof'].min(), merged_motifs['Mutability_reshmoof'].min())\n",
    "max_bound = max(merged_motifs['Mutability_shmoof'].max(), merged_motifs['Mutability_reshmoof'].max())\n",
    "\n",
    "# Add y=x line\n",
    "plt.plot([min_bound, max_bound], [min_bound, max_bound], 'r--')\n",
    "\n",
    "plt.xlabel('Shmoof Mutability')\n",
    "plt.ylabel('Reshmoof Mutability')\n",
    "plt.title('Comparison of Motif Mutabilities: Shmoof vs. Reshmoof')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `model` is your trained model instance\n",
    "log_site_rates_short = model.log_site_rates.weight.data.squeeze().cpu().numpy()\n",
    "log_site_rates_long_residual = model.log_site_rates_long_residual.weight.data.squeeze().cpu().numpy()\n",
    "\n",
    "# Calculate the exponential of the log rates\n",
    "site_rates_short = torch.exp(torch.tensor(log_site_rates_short)).numpy()\n",
    "site_rates_long_residual = torch.exp(torch.tensor(log_site_rates_long_residual)).numpy()\n",
    "\n",
    "# Ensure the site rates are 1-dimensional\n",
    "assert site_rates_short.ndim == 1\n",
    "assert site_rates_long_residual.ndim == 1\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    'Position': range(1, len(site_rates_short) + 1),\n",
    "    'Short_Site_Rates': site_rates_short,\n",
    "    'Long_Residual_Site_Rates': site_rates_long_residual\n",
    "})\n",
    "\n",
    "# Line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Position'], df['Short_Site_Rates'], label='Short Site Rates', color='orange')\n",
    "plt.plot(df['Position'], df['Long_Residual_Site_Rates'], label='Long Site Residuals', color='green', alpha=0.5)\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Mutability')\n",
    "plt.title(f'Comparison of Short vs. Long Site Rates, residual_penalty_weight={model.residual_penalty_weight}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regularization_coeffs = [0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "results = []\n",
    "\n",
    "for coeff in regularization_coeffs:\n",
    "    print(f\"Training with regularization coefficient {coeff}\")\n",
    "    model = shmoof.SHMoofModel(train_dataset)\n",
    "    burrito = shmoof.NoofBurrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, l2_regularization_coeff=1e-6)\n",
    "    loss_history = burrito.train(epochs=20)\n",
    "    final_training_loss = loss_history['training_losses'].iloc[-1]\n",
    "    final_validation_loss = loss_history['validation_losses'].iloc[-1]\n",
    "\n",
    "    results.append({\n",
    "        'Regularization': coeff,\n",
    "        'Final_Training_Loss': final_training_loss,\n",
    "        'Final_Validation_Loss': final_validation_loss\n",
    "    })\n",
    "\n",
    "regularization_results_df = pd.DataFrame(results)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(regularization_results_df['Regularization'], regularization_results_df['Final_Training_Loss'], label='Training Loss', marker='o')\n",
    "plt.plot(regularization_results_df['Regularization'], regularization_results_df['Final_Validation_Loss'], label='Validation Loss', marker='x')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Coefficient')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Effect of L2 Regularization on Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
